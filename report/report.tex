\title{60-371\\Artificial Intelligence\\Iterated Prisoner's Dilemma}
\author{
		Quinn Perfetto \\
        William Roeder \\
        David Valleau
}
\date{\today}

\documentclass[12pt]{article}

\usepackage{ amssymb }
\usepackage{longtable}
\usepackage[T1]{fontenc}
\usepackage{tocloft}
\usepackage{hyperref}
\usepackage[none]{hyphenat}
\usepackage[]{algorithm2e}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue
}
\renewcommand\cftsecleader{\cftdotfill{\cftdotsep}}
\setlength\parindent{0pt}


\begin{document}
\maketitle

\pagebreak
\tableofcontents
\pagebreak

\section{Abstract}

This paper explores the use of multiple Artificial Intelligence algorithms to
create optimal strategies to compete in the Iterated Prisoner's Dilemma.  The
study begins by examining the use of a genetic algorithm to breed the perfect
prisoner.  Several different configurations and fitness functions were tested and
the performance of each was contrasted.  The genetic algorithm was able to
consistently breed well performing prisoners.
A hill climbing approach was then taken,
which proved to be ineffective at producing a competitive strategy given the
lack of a natural successor function. Finally, machine learning... <INSERT
SUMMARY HERE>.  The source code used for this study can be found in the final
section of this paper.

\pagebreak

\section{Introduction}
The Iterated Prisoner's Dilemma is a classic example of game theory which demonstrates
that two agents acting for their own self interest may not result in an
optimal outcome for either agent.  The prisoner's
dilemma is as follows: \\ \\
Two individuals (herein Bob and Alice) have been arrested and placed in solitary
confinement so that they may not communicate with one another.  Each of them hope
to spend the least amount of time in prison.  The prosecutor approaches them
and offers the following deal:
\begin{itemize}
    \item If Bob and Alice betray (herein defect) each other, they will
        both serve 2 years in prison
    \item If Bob defects but Alice remains silent (herein cooperate), Bob
        will be set free and Alice will spend 3 years in prison (and vice versa)
    \item If Bob and Alice both cooperate they will each spend 1 year in prison
        on a lesser charge \\
\end{itemize}

The iterated version of the prisoner's dilemma is simply a sequence of rounds of
the above mentioned game.  A player's score is the summation of their score in
each round. \\

It is implied that Bob and Alice will have no interactions with each other after
their decisions and thus cannot punish/reward their accomplice. If Bob and Alice are
both rational and self centered then they will both choose to defect, as defecting
yields the highest probability of a favourable outcome.  It is interesting to note
that if the prisoners act non-rationally and choose to cooperate
(the "riskier" option) they will spend 1/3 of the time in prison compared
to the rational option. \\

\pagebreak

Taking after J. Golbeck
\footnote{\href{http://cgis.cs.umd.edu/~golbeck/downloads/JGolbeck\_prison.pdf}
{Evolving Strategies for the Prisoner's Dilemma}}
the payoff matrix for this study has been
inverted such that a higher score implies a lesser sentence. \\

\begin{center}
    \begin{tabular}{l | c | c}
         & Cooperate & Defect \\
        \hline
        Cooperate & (3,3) & (5, 0)\\
        \hline
        Defect & (0, 5) & (1,1) \\ \\
    \end{tabular}
\end{center}

Iterated prisoner's dilemma strategies have been widely studied, and many have
concluded that Tit for tat (herein TFT) produces the best average performance.
TFT is extremely simple: Cooperate on the first turn, mirror the opponents
last move thereafter.  TFT is effective because it capitalizes on mutual
cooperation, but is also able to defend itself against rouge defectors.  These
two qualities are extremely important and were kept in mind when designing the
algorithms throughout this paper.
\clearpage
\pagebreak

\section{Genetic Algorithm}

The genetic algorithm in this paper has a structure equivalent to: \\

\begin{algorithm}[H]
 \KwResult{An evolved genetic prisoner}
 population = [] \;
 \For{$i\leftarrow 1$ \KwTo $popSize$}{
    Push(population, randomPrisoner())\;
  }
 \For{$g\leftarrow 1$ \KwTo $generation$}{
  %read current\;
  evaluation = EvaluateFitness(population)\;
  selection  = WeightedRandomSample(evaluation)\;
  cross      = CrossOver(selection)\;
  population = Mutate(cross);
 }
 \KwRet{MaxFitness(population),}
\end{algorithm}

\subsection{Representation}
Each genetic prisoner's strategy is represented as a vector of size
$4^n$ where $n$ is the number of moves the prisoner keeps in memory.  This vector
is thought of as the prisoner's "genome".
The prisoner's move is then calculated by encoding the last $n$ rounds of the game
into an integer and indexing the strategy vector at that point. \\

For example if Bob has
a memory size of one, a possible strategy vector would be
$S_B = [C, C, D, C]$.  If in the previous round Bob cooperated and Alice defected,
the corresponding history string would be represented as $DC$.
The history string is then
marshalled into a base two integer by changing Defections to 0's and Cooperations
to 1's, yielding $01$.  The strategy vector is then indexed at the base
10 representation giving ${S_B}_1 = C$, thus Bob will Cooperate. \\

All examples below use a memory size of three as it was observed to produce the
best results (see observations section \textit{\ref{observations}}).

\subsection{Initial Population}
The initial population was a pool of prisoners having completely randomized
strategy vectors.  The population size was variable, and the effects of varying
sizes are explained in the observations section <OR SHOULD THAT BE HERE?>.

\subsection{Fitness Function}

Three different fitness functions were used to evaluate a prisoners performance.
A description of each as well as performance comparisons follow.

\subsubsection{Score Against Diverse Opponents}
The prisoner being evaluated would play 100 rounds against a selection of 9
strategies which were thought to be a uniform representation of all possible
strategies.  Total score was calculated and used as a performance grade.
The nine strategies were:
\begin{enumerate}
    \item All C - Always cooperate
    \item All D - Always defect
    \item Tit for Tat - Cooperate first move, mirror opponents last move thereafter
    \item Suspicious Tit for Tat - Defect first move, mirror opponents last move
        thereafter
    \item Tit for 2 Tat - Cooperate first two moves, only cooperate if the
        opponent did not defect twice in a row thereafter
    \item Suspicious Tit for 2 Tat - Defect first two moves, only cooperate if the
        opponent did not defect twice in a row thereafter
    \item Grudger - Cooperate until the opponent defects, then defect without mercy
    \item Sucker - Defect until the opponent cooperates, then cooperate foolishly
    \item Hesitant - Only cooperate if the opponent has cooperated twice in a row
\end{enumerate}

From a shallow point of view this method of evaluating fitness was mostly
successful.  After some contemplation it was decided that the
9 strategies used were slightly biased towards those which tend to cooperate.
This left some of the prisoners produced vulnerable to rouge defectors,
and thus sub-optimal.

\subsubsection{Hamming Distance From TFT Genome}
The prisoner would be evaluated based on the hamming distance
\footnote
{\href{https://en.wikipedia.org/wiki/Hamming_distance}{Hamming distance}}
between its current strategy vector and the strategy vector representing
the TFT genome.  This obviously had a tendency to produce prisoners which
acted increasingly similar to TFT.  Although the performance of said bots
was sound, this method did not produce any extraordinary or new strategies.

\subsubsection{Score Against TFT}

The prisoner would be evaluated based on its score after 100 rounds against
TFT.  Since TFT is widely accepted as one of the best overall strategies it was
decided that performance against it would be a suitable benchmark.
This method seemed to produce the best results with the least
amount of computation time.  The prisoners produced behaved somewhat similarly
to TFT but contained some genome sections which differed considerably.  The result
is a prisoner which \textit{mostly} contains the desired qualities from TFT
but also includes some routines which give it an advantage in certain situations.

<INSERT GRAPHS AND COMPARISONS HERE>

\subsection{Selection}

After evaluating fitness (explained in the previous section) a pool of
prisoners were selected to survive into the next generation.  This pool was
selected using weighted random sampling.  In order to accomplish this the
discrete cumulative density function
\footnote{\href{https://en.wikipedia.org/wiki/Cumulative_distribution_function}{CDF}}
, in this case an array of the cumulative sums of each prisoner's fitness value,
was calculated.  A random integer was then calculated as $p = Rand \in [0, CDF_n]$,
and binary such was performed to find the first element in the cumulative CDF array that
was greater than or equal to $p$.  The prisoner corresponding to this element
is then added to the next generation.  This process is repeated $pop\_size$ times
and the next generation is formed. \\

This selection method was modeled after Roulette-wheel selection via stochastic acceptance
\footnote{\href{http://arxiv.org/pdf/1109.3627.pdf}{Roulette-wheel selection}}
but has been optimized for practical usage by reducing time and space complexities.

\subsection{Cross Over}

The cross over process is trivially implemented.  Given two genomes $G_1, G_2$,
a random integer $c = Rand \in [0, min(length(G_1), length(G_2))]$ is generated
to be used as the cross over point.  Two new genomes are then given as:\\

$G'_1 = {G_1}_{[0,c)}{G_2}_{[c, length(G_2)]}$ and
$G'_2 = {G_2}_{[0, c)}{G_1}_{[c, length(G_1)]}$ \\

This cross over is applied to each adjacent pair of prisoners in the current
generation.  Since the order of the prisoners is random it follows that the pairings
are also random.

\subsection{Mutation}

Mutating a genome is also straightforward.  Each gene of the genome is changed
to a random decision $d = Rand \in [D, C]$ with a probability $p$.  Generally
this value of $p$ is low (between 1 and 5 percent) to avoid creating unstable
evolution while still introducing some element of randomness.

\pagebreak

\section{Hill Climbing}

\pagebreak

\section{Machine Learning}

\pagebreak

\section{Observations}
\label{observations}

\pagebreak

\section{Conclusion}

\pagebreak

\section{Source Code}
All source code for this study was written in C++ and is
available on
\href{https://github.com/Quinny/IteratedPrisoners}{Github}

\end{document}
